{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Regression (2) - 2 dimensional regression.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paryoja/deeplearningstudy/blob/master/Regression_(2)_2_dimensional_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "TLdQLky7mFiO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 실습\n",
        "\n",
        "2차원 데이터의 linear regression을 해보자\n",
        "\n",
        "입력은 dx1, dx2\n",
        "\n",
        "맞춰야 하는 값은 dy 이다"
      ]
    },
    {
      "metadata": {
        "id": "NYgkOh28lwDe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline \n",
        "\n",
        "_x1 = np.arange(20, dtype=np.float32)\n",
        "_x2 = np.arange(20, dtype=np.float32)\n",
        "\n",
        "dx1 = []\n",
        "dx2 = []\n",
        "dy = []\n",
        "for x1 in _x1:\n",
        "    for x2 in _x2:\n",
        "        dx1.append(x1)\n",
        "        dx2.append(x2)\n",
        "        dy.append(x1 * 3 + x2 * 5 - 1 + np.random.normal(0, 5))\n",
        "\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "ax.scatter(dx1, dx2, dy, c='r')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cxnpBFxOmVBY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 답안\n",
        "\n",
        "먼저 어느 함수를 통해서 regression을 할 것인지를 정해야한다.\n",
        "\n",
        "w1, w2, b 의 변수를 이용해서\n",
        "\n",
        "w1 * x1 + w2 * x2 + b 라는 함수로 예측해보자"
      ]
    },
    {
      "metadata": {
        "id": "iyHxYKH4mGxW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "w1 = tf.Variable(tf.random_normal([1]), name='w1')\n",
        "w2 = tf.Variable(tf.random_normal([1]), name='w2')\n",
        "b = tf.Variable(tf.zeros([1]), name='b')\n",
        "\n",
        "x1 = tf.constant(dx1)\n",
        "x2 = tf.constant(dx2)\n",
        "y = tf.constant(dy)\n",
        "\n",
        "# 모델을 만드는 핵심 부분\n",
        "hypothesis = w1 * x1 + w2 * x2 + b\n",
        "\n",
        "# loss는 squared error sum을 사용했다\n",
        "loss = tf.reduce_sum(tf.square(hypothesis - y))\n",
        "\n",
        "learning_rate = 0.00001\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss)\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for i in range(100):\n",
        "  # _ 도 합법적인 변수의 이름이다.\n",
        "  # 파이썬에서는 무시할 변수가 있다면 그 변수의 이름 _로 한다\n",
        "  # train_op 의 sess.run 결과는 None이기 때문에 굳이 필요 없다\n",
        "  l, _ = sess.run([loss, train_op])\n",
        "  \n",
        "  print('epoch',i, 'loss', l)\n",
        "\n",
        "w1_value, w2_value, b_value = sess.run([w1, w2, b])\n",
        "\n",
        "prediction = w1_value * dx1 + w2_value * dx2 + b_value\n",
        "\n",
        "\n",
        "if np.isnan(prediction[0]):\n",
        "  print(\"Prediction is Nan!!\")\n",
        "else:  \n",
        "  fig = plt.figure()\n",
        "  ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "  ax.scatter(dx1, dx2, dy, c='r')\n",
        "  ax.scatter(dx1, dx2, prediction)\n",
        "  plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "am7vms6QorFO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Learning rate 에 따른 수렴속도 차이를 그려보자\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Tew8eN6Gno9s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "w1 = tf.Variable(tf.random_normal([1]), name='w1')\n",
        "w2 = tf.Variable(tf.random_normal([1]), name='w2')\n",
        "b = tf.Variable(tf.zeros([1]), name='b')\n",
        "\n",
        "x1 = tf.constant(dx1)\n",
        "x2 = tf.constant(dx2)\n",
        "y = tf.constant(dy)\n",
        "\n",
        "hypothesis = w1 * x1 + w2 * x2 + b\n",
        "\n",
        "loss = tf.reduce_sum(tf.square(hypothesis - y))\n",
        "\n",
        "\n",
        "# 비교할 learning rate의 리스트\n",
        "learning_rate_list = [0.00001, 0.000001, 0.0000001]\n",
        "\n",
        "# 각 learning_rate 마다 loss의 변화를 기록할 리스트\n",
        "loss_per_rate = []\n",
        "\n",
        "for learning_rate in learning_rate_list:\n",
        "  #각 learning_rate 마다 optimizer를 새로 만들어줘야함\n",
        "  \n",
        "  optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "  train_op = optimizer.minimize(loss)\n",
        "\n",
        "  sess = tf.Session()\n",
        "  \n",
        "  # random 함수를 이용하므로 돌릴때마다 초기 값이 달라지는 것을 막기 위해 \n",
        "  # 같은 seed 값으로 설정\n",
        "  tf.set_random_seed(1000)\n",
        "\n",
        "  # 실제 초기화가 되는 부분\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  \n",
        "  # 같은 값으로 됬는지 확인 하기 위해 찍어봄\n",
        "  print(sess.run([w1, w2, b]))\n",
        "  \n",
        "  # 이 learning rate에서의 loss 의 변화를 기록할 리스트\n",
        "  loss_list = []\n",
        "  \n",
        "  for i in range(100):\n",
        "    # _ 도 합법적인 변수의 이름이다.\n",
        "    # 파이썬에서는 무시할 변수가 있다면 그 변수의 이름 _로 한다\n",
        "    # train_op 의 sess.run 결과는 None이기 때문에 굳이 필요 없다\n",
        "    \n",
        "    l, _ = sess.run([loss, train_op])\n",
        "    \n",
        "    # 계산된 loss값을 저장한다.\n",
        "    loss_list.append(l)\n",
        "\n",
        "  w1_value, w2_value, b_value = sess.run([w1, w2, b])\n",
        "\n",
        "  loss_per_rate.append(loss_list)\n",
        "\n",
        "  if np.isnan(prediction[0]):\n",
        "    print(\"Prediction is Nan!!\")\n",
        "    break\n",
        "\n",
        "# 사용할 color의 리스트\n",
        "color_list = ['r', 'g', 'b']\n",
        "\n",
        "# 파이썬 내장함수인 zip과 enumerate를 사용했는데, \n",
        "# 익숙하지 않다면 인터넷 검색을 해서 사용법을 익히도록 하자\n",
        "# 매우 빈번히 쓰이는 함수이다\n",
        "\n",
        "for i, (lr, color) in enumerate(zip(learning_rate_list, color_list)):\n",
        "  print(lr, color)\n",
        "  plt.plot(range(100), loss_per_rate[i], color, label=lr)\n",
        "\n",
        "plt.legend(loc='upper right')\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kOC94MaBoqt5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Matrix 사용하기\n",
        "\n",
        "\n",
        "hypothesis = w1 * x1 + w2 * x2 + b 를 벡터 표현으로 써보자\n",
        "\n",
        "W를 (1, 2) 행렬 \n",
        "\n",
        "```\n",
        "[[w1, w2]]\n",
        "```\n",
        "\n",
        "x를 열벡터 \n",
        "```\n",
        "[x1,\n",
        " x2]\n",
        "```\n",
        "\n",
        "라고 하면\n",
        "W 와 x의 행렬곱은\n",
        "\n",
        " w1 * x1 + w2 * x2 가된다\n",
        " \n",
        " 즉,\n",
        " \n",
        " hypothesis는 W x + b \n",
        " \n",
        " 로 표현 가능하며,\n",
        " \n",
        " 이렇게 행렬과 벡터의 곱으로 구성할때, 아무런 연산자 표시가 없으면 행렬곱을 의미한다.\n",
        " \n",
        " \n",
        "\n",
        "\n",
        "W를 \n",
        "\n",
        "\n",
        "```\n",
        "[[w1, w2, b]]\n",
        "```\n",
        "\n",
        "x를 열벡터 \n",
        "```\n",
        "[x1,\n",
        " x2,\n",
        " 1]\n",
        "```\n",
        "로 표현한다면,\n",
        "\n",
        "\n",
        "hypothesis 를 W x 로 표시하는 것도 가능하다.\n"
      ]
    },
    {
      "metadata": {
        "id": "RZu1ncWstf0q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "x = tf.constant([dx1, dx2]) \n",
        "\n",
        "# dx1은 (400, ), dx2는 (400, ) 이므로, (400, ) 가 두개가 들어 있는 matrix \n",
        "# 즉, (2, 400) 이다\n",
        "\n",
        "W = tf.Variable(tf.random_normal([1,2]), name='W')\n",
        "b = tf.Variable(tf.zeros([1]), name='b')\n",
        "\n",
        "\n",
        "y = tf.constant(dy)\n",
        "\n",
        "# 다음 연산 결과는 1, 400 이다\n",
        "hypothesis = tf.matmul(W, x) + b\n",
        "\n",
        "loss = tf.reduce_sum(tf.square(hypothesis - y))\n",
        "\n",
        "learning_rate = 0.00001\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss)\n",
        "\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "\n",
        "# 실행하기 전에 위에서 설명한대로 x의 shape을 살펴보자\n",
        "# 텐서의 shape을 보기위해서는 tf.shape 함수를 사용한다\n",
        "\n",
        "print('shape of x', sess.run(tf.shape(x)))\n",
        "\n",
        "\n",
        "for i in range(100):\n",
        "  l, _ = sess.run([loss, train_op])\n",
        "  \n",
        "  # print('epoch',i, 'loss', l)\n",
        "\n",
        "W_value, b_value = sess.run([W, b])\n",
        "\n",
        "# numpy에서도 matrix multiplication은 matmul이다\n",
        "prediction = np.matmul(W_value,  [dx1, dx2]) + b_value\n",
        "\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "ax.scatter(dx1, dx2, dy, c='r')\n",
        "ax.scatter(dx1, dx2, prediction)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lL4l-7txxbQv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "다수의 강의에서는 벡터라 함은,\n",
        "\n",
        "데이터를 세로로 나열한 column vector를 기준으로 하고 있다.\n",
        "\n",
        "위에서 사용한 \n",
        "\n",
        "```\n",
        "[dx1, dx2]\n",
        "```\n",
        "를 찍어보면"
      ]
    },
    {
      "metadata": {
        "id": "U2hArY6Sx4Rp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(np.array([dx1, dx2]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MzfvpHLKx_g0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "잠깐 우리의 데이터를 생각해보자.\n",
        "\n",
        "데이터 생성은\n",
        "\n",
        "```\n",
        "x1 * 3 + x2 * 5 - 1 + np.random.normal(0, 5)\n",
        "```\n",
        "\n",
        "의 코드를 통해서 했다.\n",
        "\n",
        "\n",
        "\n",
        "우리는  (0, 0) 일 때, -1 + noise\n",
        "\n",
        "(0, 1) 일 때는 4 + noise\n",
        "\n",
        "(0, 2) 일 때는 9 + noise\n",
        "\n",
        "\n",
        "값이 나오도록 데이터를 만들었는데,\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "위 데이터를 보면,  x 값으로 몇을 썼는지 보기가 불편하다.\n",
        "\n",
        "사람이 코딩 하기에는 \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "2czsotrtyu73",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(np.transpose([dx1, dx2])[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ko3HFC-Wy4O5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "와 같이 row 벡터를 쓰는 것이 더욱 직관적이다.\n",
        "\n",
        "따라서, 이후 실습에서는 row 벡터를 기준으로 데이터를 표현할 것이다.\n",
        "\n",
        "\n",
        "달라지는 점은\n",
        "\n",
        "hypothesis 가 W x 로 계산되는것 이 아니라\n",
        "\n",
        "x W 로 계산된다.\n",
        "\n",
        "또한 여기서 W는\n",
        "```\n",
        "[[w1]\n",
        " [w2]]\n",
        "```\n",
        "의 형태로 이루어진다.\n",
        "\n",
        "row 벡터를 기준으로 구현하면 다음과 같다"
      ]
    },
    {
      "metadata": {
        "id": "IyI1Et1jzF85",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "x = tf.constant(np.transpose([dx1, dx2]))\n",
        "\n",
        "# dx1은 (400, ), dx2는 (400, ) 이므로, (400, ) 가 두개가 들어 있는 matrix \n",
        "# 즉, (2, 400) 이다\n",
        "\n",
        "W = tf.Variable(tf.random_normal([2, 1]), name='W')\n",
        "b = tf.Variable(tf.zeros([1]), name='b')\n",
        "\n",
        "\n",
        "y = tf.constant(dy)\n",
        "\n",
        "# 다음 연산 결과는 400, 1 이다\n",
        "hypothesis = tf.matmul(x, W) + b\n",
        "\n",
        "loss = tf.reduce_sum(tf.square(hypothesis - y))\n",
        "\n",
        "learning_rate = 0.00000001\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss)\n",
        "\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "\n",
        "# 실행하기 전에 위에서 설명한대로 x의 shape을 살펴보자\n",
        "# 텐서의 shape을 보기위해서는 tf.shape 함수를 사용한다\n",
        "\n",
        "print('shape of x', sess.run(tf.shape(x)))\n",
        "\n",
        "\n",
        "for i in range(10000):\n",
        "  l, _ = sess.run([loss, train_op])\n",
        "  if i % 1000 == 0:\n",
        "    print('epoch',i, 'loss', l)\n",
        "\n",
        "W_value, b_value = sess.run([W, b])\n",
        "\n",
        "# numpy에서도 matrix multiplication은 matmul이다\n",
        "prediction = np.matmul(np.transpose([dx1, dx2]), W_value) + b_value\n",
        "\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "ax.scatter(dx1, dx2, dy, c='r')\n",
        "ax.scatter(dx1, dx2, prediction)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tkgbi_6RwLfI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "위의 수행 결과를 보면,\n",
        "\n",
        "loss는 수렴하여 값이 고정 되었으나,\n",
        "\n",
        "실제 그려보니 전혀 다른 형태임을 보이고 있다.\n",
        "\n",
        "위에서 문제가 된 버그를  찾아서 고쳐보자\n",
        "\n",
        "힌트 (뺄셈에서의 broadcasting)"
      ]
    },
    {
      "metadata": {
        "id": "fvL8Nnqs56g_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Tensorflow 내에서 transpose를 하기 위해서는\n",
        "\n",
        "tf.transpose 함수를 이용 할 수 있다.\n",
        "\n",
        "\n",
        "\n",
        "행 벡터나 열 백터를 transpose하는 것은 reshape 하는 것과 같다"
      ]
    },
    {
      "metadata": {
        "id": "0AB1MdJLuR47",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline \n",
        "\n",
        "_x1 = np.arange(20, dtype=np.float32)\n",
        "_x2 = np.arange(20, dtype=np.float32)\n",
        "\n",
        "dx1 = []\n",
        "dx2 = []\n",
        "dy = []\n",
        "for x1 in _x1:\n",
        "    for x2 in _x2:\n",
        "        dx1.append(x1)\n",
        "        dx2.append(x2)\n",
        "        dy.append(x1 * 3 + x2 * 5 - 1 + np.random.normal(0, 5))\n",
        "\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "ax.scatter(dx1, dx2, dy, c='r')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "x = tf.constant(np.transpose([dx1, dx2]))\n",
        "\n",
        "# dx1은 (400, ), dx2는 (400, ) 이므로, (400, ) 가 두개가 들어 있는 matrix \n",
        "# 즉, (2, 400) 이다\n",
        "\n",
        "W = tf.Variable(tf.random_normal([2, 1]), name='W')\n",
        "b = tf.Variable(tf.zeros([1]), name='b')\n",
        "\n",
        "\n",
        "y = tf.constant(dy)\n",
        "\n",
        "# 다음 연산 결과는 400, 1 이다\n",
        "hypothesis = tf.matmul(x, W) + b\n",
        "\n",
        "loss = tf.reduce_sum(tf.square(hypothesis - tf.reshape(y, [400,1])))\n",
        "\n",
        "learning_rate = 0.000001\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss)\n",
        "\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "\n",
        "# 실행하기 전에 위에서 설명한대로 x의 shape을 살펴보자\n",
        "# 텐서의 shape을 보기위해서는 tf.shape 함수를 사용한다\n",
        "\n",
        "print('shape of x', sess.run(tf.shape(x)))\n",
        "\n",
        "\n",
        "for i in range(10000):\n",
        "  l, _ = sess.run([loss, train_op])\n",
        "  if i % 1000 == 0:\n",
        "    print('epoch',i, 'loss', l)\n",
        "\n",
        "W_value, b_value = sess.run([W, b])\n",
        "\n",
        "# numpy에서도 matrix multiplication은 matmul이다\n",
        "prediction = np.matmul(np.transpose([dx1, dx2]), W_value) + b_value\n",
        "\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "ax.scatter(dx1, dx2, dy, c='r')\n",
        "ax.scatter(dx1, dx2, prediction)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}