{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "다음과 같은 데이터를 사용해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.arange(10, dtype=np.float32)\n",
    "\n",
    "y = x + np.random.normal(0, 0.5, 10)\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "plt.plot(x, y, '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 데이터를 fitting 하는 linear model을 구현해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "input_x = tf.constant(x, dtype=tf.float32)\n",
    "target_y = tf.constant(y, dtype=tf.float32)\n",
    "\n",
    "w = tf.Variable(tf.random_normal([1]))\n",
    "b = tf.Variable(0.0)\n",
    "\n",
    "hypothesis = w * input_x + b\n",
    "\n",
    "\n",
    "loss = tf.reduce_sum(tf.square(hypothesis - target_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent 로 optimize 해보자\n",
    "\n",
    "tf.train.GradientDescentOptimizer\n",
    "\n",
    "class의 instant로 할 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "\n",
    "train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_op 에는 먼저\n",
    "\n",
    "1. loss의 변수에 대한 gradient를 계산한다\n",
    "\n",
    "2. 변수 w를 w - learning_rate * gradient_of_w 로 업데이트 한다 (b도 마찬가지)\n",
    "\n",
    "두가지로 이루어져 있다\n",
    "\n",
    "여기서 다시 tensorflow는 게으름을 생각해보자.\n",
    "\n",
    "여기까지는 어떻게 할 거다 만 정의한 것이고,\n",
    "\n",
    "실제로 수행은 sess 하나가 필요하다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "sess.run(train_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "굉장한 에러가 났다\n",
    "\n",
    "무엇이 빠졌는지 다음 셀에 채워보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "## 여기에 필요한 연산을 넣어보자\n",
    "\n",
    "print(sess.run([loss, w, b]))\n",
    "sess.run(train_op)\n",
    "print(sess.run([loss, w, b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한번 수행하면 어떻게 됬는지 보자 w와 b값은 어떻게 됬을까?\n",
    "\n",
    "우리가 예상한 값에서 아직 많이 멀다.\n",
    "\n",
    "한 백번정도 업데이트를 시켜보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    sess.run(train_op)\n",
    "    print(sess.run([loss, w, b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "찾아진 w 와 b로 그래프를 그려보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_w, final_b = sess.run([w, b])\n",
    "\n",
    "plt.plot(x, y, '.')\n",
    "plt.plot(x, final_w * x + final_b, '.')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, y, '.')\n",
    "\n",
    "new_x = np.linspace(0, 10, 1000)\n",
    "plt.plot(new_x, final_w * new_x + final_b, '.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실습\n",
    "\n",
    "2차원 데이터의 linear regression을 해보자\n",
    "\n",
    "입력은 dx1, dx2\n",
    "\n",
    "맞춰야 하는 값은 dy 이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_x1 = np.arange(20)\n",
    "_x2 = np.arange(20)\n",
    "\n",
    "dx1 = []\n",
    "dx2 = []\n",
    "dy = []\n",
    "for x1 in _x1:\n",
    "    for x2 in _x2:\n",
    "        dx1.append(x1)\n",
    "        dx2.append(x2)\n",
    "        dy.append(x1 * 3 + x2 * 5 - 1 + np.random.normal(0, 5))\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(dx1, dx2, dy, c='r')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 과제\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "없이\n",
    "\n",
    "실제로 gradient descent를\n",
    "\n",
    "손으로 미분한 것과\n",
    "\n",
    "tf.assign을 활용하여 구현해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
